                <meta charset="utf-8" emacsmode="-*- markdown -*-">
                  **Simulating a plane mirror using 2D images for the TWM**
                            		Nkiruka Uzuegbunam

Virtual Plane Mirror simulation using 2D color images
===============================================================================

This post describes a method for simulating a plane mirror using 2D images of the background in the robotic mirror setup described in earlier posts. I detail the mathematics, 
show some simulation results, and discuss methods of improvement in various aspects of the system. In the end, the work boils down to appropriately scaling the background according
to the user's distance from the mirror. 

[Click here](../index.html) to navigate back to home page.

Previous Work in virtual mirror simulation
-------------------------------------------------------------------------------
Mirrors are used in many different applications, ranging from everyday personal care to medical use. However, they are not interactive. Virtual mirrors are interactive mirrors. Typically, they consist of a display with static depth or rgb cameras. The cameras track the user and/or the environment, creating a reflection based interaction. Often, the user views their reflections augmented with virtual objects. For instance, in virtual mirrors for fashion, the virtual objects are sunglasses, clothes, shoes, etc. In medicine, the virtual objects could be health indicators of some sort. 

However, many virtual mirror display systems do not strive to be true mirrors, depicting a correct reflection based on the user's perspective. This can reduce the realism in the display. Several previous works have tackled creating proper reflection in various virtual mirror systems. [#Francois13] created a handheld virtual mirror as a prototype for geometrically correct reflections in virtual mirrors. Ignoring the user's background, they used a single static rgb camera, and magnetic sensors tracking the user's location to create their mirror reflections. With the advent of depth cameras, [#Shen13], [#Shen12], [#Straka11], and [#Su17] create virtual mirror reflections with a network of one or more depth and rgb cameras, with [#Su17] creating mirror reflections in non-planar mirror settings. [#Lee17] is a direct extension of the work of [#Francois13] in onscreen reflections as a navigation tool in camera-based systems.

In this work, we extend the work of [#Francois13] in two important ways. First, we take the user's interaction with the background as an important component of interacting with a mirror device. Second, we introduce a novel robotic system that tracks the user's location. 

Estimating the Mirror Reflection
-------------------------------------------------------------------------------
The system of Francois and Kang [#Francois13] proposed a camera and screen (with magnetic sensors for user position tracking) for simulating a virtual mirror with geometrically correct reflections. In particular, their model is derived in four phases based on three key assumptions. In the first phase, objects are mapped from the 3D world to the mirror plane. Let an object be located at point P in the virtual viewer's coordinate system, Figure [virtualViewpt]. P is the point $[ X, Y, Z ]^\T$. It is mapped to a point, p, on the mirror surface $[xp, yp, d]^\T$ where d is the orthogonal distance from the virtual viewer to the mirror 
plane: 

![Figure [virtualViewpt]: The virtual user's viewpoint from [#Francois13]](../assets/figure2_KangFrancois.png)

\begin{equation}
 \begin{bmatrix}
 X_p \\
 Y_p \\
 Z_p 
 \end{bmatrix}
 = 
 \begin{bmatrix}
 d & 0 & 0 & 0\\
 0 & d & 0 & 0 \\
 0 & 0 & 1 & 0
 \end{bmatrix}
 * 
  \begin{bmatrix}
 X \\
 Y \\
 Z \\
 1  
 \end{bmatrix}
\end{equation}

Then $x_p = \frac{X_p}{Z_p} = \frac{d*X}{Z}$, and $y_p = \frac{Y_p}{Z_p} = \frac{d*Y}{Z}$

In the second phase, objects are mapped from the camera's world view into the camera's imaging plane as shown in Figure [cameraViewpt]. Two key assumptions are made here: 1) the camera is assumed to be a pinhole camera and 2) the imaging plane of the camera is assumed to coincide with that of the mirror plane. As such, we have that a point $P_c = [ X_c, Y_c, Z_c ]^\T$ in the camera coordinate system, with origin at camera center, maps to the point $p_c = [x_{pc}, y_{pc}, f]^\T$. f is the focal length of the camera. 

![Figure [cameraViewpt]: The camera's viewpoint from [#Francois13]](../assets/figure3_KangFrancois_a.png)

\begin{equation}
 \begin{bmatrix}
 X_{pc} \\
 Y_{pc} \\
 Z_{pc} 
 \end{bmatrix}
 = 
 \begin{bmatrix}
 f & 0 & 0 & 0\\
 0 & f & 0 & 0 \\
 0 & 0 & 1 & 0
 \end{bmatrix}
 * 
  \begin{bmatrix}
 X_c \\
 Y_c \\
 Z_c \\
 1  
 \end{bmatrix}
\end{equation}

Then $x_{pc} = \frac{X_{pc}}{Z_{pc}} = \frac{f*X_c}{Z_c}$, and $y_{pc} = \frac{Y_{pc}}{Z_{pc}} = \frac{f*Y_c}{Z_c}$

The third phase consists of mapping the camera viewpoint to the virtual user's viewpoint. The key assumption here is that the image and virtual mirror planes coincide. Hence, the camera's origin is mapped to the virtual user's viewpoint via a simple translation $[t_x, t_y, t_z]^\T$ as seen in Figure [viewpt]. Then the point p in the virtual user viewpoint is related to the a point $P_c$ in the camera coordinate system as:


![Figure [viewpt]: The camera and virtual user's viewpoints from [#Francois13]](../assets/figure3_KangFrancois_b.png)


\begin{equation}
  \begin{bmatrix}
X_p \\
 Y_p \\
 Z_p 
 \end{bmatrix}
 = 
 \begin{bmatrix}
 d & 0 & 0 & 0\\
 0 & d & 0 & 0 \\
 0 & 0 & 1 & 0
 \end{bmatrix}
 * 
  \begin{bmatrix}
 X \\
 Y \\
 Z \\
 1  
 \end{bmatrix}
= 
 \begin{bmatrix}
 d & 0 & 0 & 0\\
 0 & d & 0 & 0 \\
 0 & 0 & 1 & 0
 \end{bmatrix}
 * 
 \begin{bmatrix}
 1 & 0 & 0 & t_x\\
 0 & 1 & 0 & t_y \\
 0 & 0 & 1 & t_z \\
 0 & 0 & 0 & 1 
 \end{bmatrix}
 * 
  \begin{bmatrix}
 X_c \\
 Y_c \\
 Z_c \\
 1  
 \end{bmatrix}
\end{equation}

Then a point in the virtual user's mirror plane, $p = [x_p, y_p]^\T$, is defined as: 
\begin{equation}
x_p = \frac{X_p}{Z_p} = \frac{d*X}{Z} = \frac{d * (X_c + t_x)}{Z_c + t_z}, \\
y_p = \frac{Y_p}{Z_p} = \frac{d*Y}{Z} = \frac{d * (Y_c + t_y)}{Z_c + t_z}
\end{equation}

In order to relate the point p in the virtual user's mirror plane to the point $p_c$ in the camera's image plane, it would be necessary to obtain 3D positions of each point in the environment. However, Francois and Kang [#Francois13] assume that as the user is the center of interest, and **not the background**, the world may be modeled as a plane parallel to the mirror plane, at the same distance as the user's viewpoint. This is termed Flat world assumption, and is shown in Figure [worldAsPlane]. Hence, the z-component of the camera coordinate system is fixed as $Z_c = f + d$. Thus, we relate p, to $p_c$ as: 

![Figure [worldAsPlane]: Flat world assumption from [#Francois13]](../assets/figure5_KangFrancois.png)

\begin{equation}
x_p = \frac{X_p}{Z_p} = \frac{d*X}{Z} = \frac{d * (X_c + t_x)}{Z_c + t_z} = \frac{d * ( \frac{Z_c * x_{pc}}{f} + t_x)}{Z_c + t_z} =   \frac{d * ( \frac{ (f+d) * x_{pc}}{f} + t_x)}{ (f+d) + t_z}\\
y_p = \frac{Y_p}{Z_p} = \frac{d*Y}{Z} = \frac{d * (Y_c + t_y)}{Z_c + t_z} = \frac{d * ( \frac{Z_c * y_{pc}}{f} + t_y)}{Z_c + t_z} =   \frac{d * ( \frac{ (f+d) * x_{pc}}{f} + t_x)}{ (f+d) + t_z}
\end{equation}


Advantages of the RoboMirror system
-------------------------------------------------------------------------------
The three key assumptions of the Francois and Kang [#Francois13] system is that 


Estimating the distance of user to Mirror
-------------------------------------------------------------------------------




Summary
-------------------------------------------------------------------------------
In this work, we have explained how a TWM works. A TWM, designed for privacy, reflects most of the light hitting it, while allowing others to pass through. It is composed of a transparent medium, often glass, acrylic or some transparent plastic, coated with a very thin layer of a reflective medium, often silver or aluminium. A TWM acts like a mirror when the reflective side is much more brightly lit than the transparent side. In my next post, I will further explain the robotic mirror, my intended usage of the TWM.


[Click here](../index.html) to navigate back to home page.

**Bibliography**:
[#Francois13]: François, A. R., and Kang, E. Y. (2003, July). A handheld mirror simulation. In Multimedia and Expo, 2003. ICME'03. Proceedings. 2003 International Conference on (Vol. 2, pp. II-745). IEEE.

[#Lee17]: Lee, J. I., Kim, S., Fukumoto, M., & Lee, B. (2017, October). Reflector: Distance-Independent, Private Pointing on a Reflective Screen. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology (pp. 351-364). ACM.

[#Shen13]: Shen, J., Su, P. C., Cheung, S. C. S., & Zhao, J. (2013). Virtual mirror rendering with stationary rgb-d cameras and stored 3-d background. IEEE Transactions on Image Processing, 22(9), 3433-3448.

[#Shen12]: Shen, J., Cheung, S. S., & Zhao, J. (2012, December). Virtual mirror by fusing multiple RGB-D cameras. In Signal & Information Processing Association Annual Summit and Conference (APSIPA ASC), 2012 Asia-Pacific (pp. 1-9). IEEE.

[#Straka11]: Straka, M., Hauswiesner, S., Rüther, M., & Bischof, H. (2011). A free-viewpoint virtual mirror with marker-less user interaction. Image Analysis, 635-645.

[#Su17]: Su, P. C., Xu, W., Shen, J., & Cheung, S. C. S. (2017, July). Real-time rendering of physical scene on virtual curved mirror with RGB-D camera networks. In Multimedia & Expo Workshops (ICMEW), 2017 IEEE International Conference on (pp. 79-84). IEEE.





<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
